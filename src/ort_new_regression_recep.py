# -*- coding: utf-8 -*-
"""Ort_new_regression-recep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UIEZWAx66rs2eHAydPt0yhw9Sg7P5W-j

# Ortaokul Notlarının Backward Elimination İle Değişken Seçimi Ve Regresyon Modelleri
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import plot_confusion_matrix
#from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV

os.chdir("D:\Belgeler\Çalışmalar\Bitirme-MEB")


data_file = "Data/Veriseti_Ortaokullar_GONDERILEN.xlsx"
data_original = pd.read_excel(data_file, engine="openpyxl")
df = data_original.copy()
#df = df.drop(columns=["okuladi", "okulno"])
df = df.drop(["okuladi","okulno"], axis=1)
print(df.isnull().any())
df.iloc[:, -3:] = df.iloc[:, -3:].astype("int64")

df.dtypes

def sifir_bir_duzelt(degisken):
  for i in df[degisken].index:
    if df[degisken][i] ==0:
      df[degisken][i] = 1
    elif df[degisken][i]==1:
      df[degisken][i]=0

sifir_bir_duzelt("Asag")
sifir_bir_duzelt("Bsag")
sifir_bir_duzelt("Aoz")
sifir_bir_duzelt("Boz")
sifir_bir_duzelt("ABayri")
sifir_bir_duzelt("Abirlikte")
sifir_bir_duzelt("Acalisma")
sifir_bir_duzelt("Bcalisma")
sifir_bir_duzelt("oda")
sifir_bir_duzelt("hastalik")
sifir_bir_duzelt("okul_dyk")
sifir_bir_duzelt("ozel_kurs")

"""## Standardization"""

# Get column names first
names = df.columns
# Create the Scaler object
scaler = preprocessing.StandardScaler()
# Fit your data on the scaler object
scaled_df = scaler.fit_transform(df)
scaled_df = pd.DataFrame(scaled_df, columns=names)

scaled_df

"""## Ort5 Regresyon Modeli"""

df_data = scaled_df.drop(["ort6", "ort7"], axis=1)

x = scaled_df.drop(["ort5", "ort6", "ort7"], axis=1)
y = scaled_df["ort5"]

def backward_elimination(data, target,significance_level = 0.05):
  features = data.columns.tolist()
  while(len(features)>0):
    features_with_constant = sm.add_constant(data[features])
    p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]
    max_p_value = p_values.max()

    if(max_p_value >= significance_level):
      excluded_feature = p_values.idxmax()
      features.remove(excluded_feature)
    else:
      break 
  return features

ort5_futures = backward_elimination(x, y)

ort5_futures

"""### Regresyon Analizi"""

x = df_data[ort5_futures]
y = df_data["ort5"]

"""İstatistik modellerini kullandığımız için, sabit terimi X değerine eklememiz gerekiyor."""

x = sm.add_constant(x)

train_x, test_x, train_y, test_y = train_test_split(x, y, train_size = 0.8, random_state = 42)

model = sm.OLS(y, x)
model = model.fit()
print(model.summary2())

"""Değişkenlerin sonuca etki yüzdelerini inceleyelim;"""

coefs = model.params
coefs = pd.DataFrame(coefs, columns=["coefficients"])
coefs.drop(index=["const"], inplace=True)
coefs

coef_sum = coefs.coefficients.abs().sum()
coefs_abs = coefs.coefficients.abs().values
coefs_abs

percentages = []
for i in coefs_abs:
  perc = i*100/coef_sum
  percentages.append(perc)

coefs["Percentages"] = percentages
coefs.sort_values(by="Percentages", ascending=False, inplace=True)
print(coefs)

"""### Split Data"""

x = df[ort5_futures]
#x=x.drop(columns=['gelir'])
y = df["ort5"]

y

train_x, val_x, train_y, val_y = train_test_split(x, y, random_state=42)

train_x

"""### Specify and Fit the Model"""

from sklearn.svm import SVR
model =SVR(C= 1000, gamma= 0.0001, kernel= 'rbf')

accuracies=cross_val_score(model,x,y,cv=5)
print("Accuracy (mean): %", accuracies.mean()*100)
print("std: %", accuracies.std()*100)
scores=cross_val_score(model,x,y,scoring='neg_mean_absolute_error',cv=5)
print("MAE (mean): %",scores.mean())

model.fit(train_x, train_y)

import pickle
model_path = "ort5.pickle"
pickle.dump(model, open(model_path, "wb"))

"""###  Make Predictions with Validation Data"""

load_model_path="ort5.pickle"
loaded_model = pickle.load(open(load_model_path, "rb"))

val_predictions = loaded_model.predict(val_x)

print("Eğitim doğruluğu: ", model.score(train_x, train_y)*100)
print("Test doğruluğu: ", model.score(val_x, val_y)*100)

val_predictions

#val_predictions = scaler_valy.inverse_transform(val_predictions)
val_predictions = val_predictions.astype("int64")
val_predictions

"""### Modelin manuel test edilmesi"""

load_model_path="ort5.pickle"
loaded_model = pickle.load(open(load_model_path, "rb"))

val_x.head()

val_y.head()

girdiler = [0,1,1,5,4,1,3,4,2,3,3,2,0,0]
data_girdi=pd.DataFrame(columns=['cinsiyet','Bsag','Aoz','Aogrenim','Bogrenim','Bcalisma','gelir', 'kardes','uyku','televizyon','eba_tv','ders_calisma','okul_dyk','sosyal_kulturel',])
data_girdi = data_girdi.append({'cinsiyet': girdiler[0], 'Bsag': girdiler[1], 'Aoz': girdiler[2], 'Aogrenim': girdiler[3], 'Bogrenim': girdiler[4], 'Bcalisma': girdiler[5], 'gelir': girdiler[6],
                                'kardes': girdiler[7],
                                'uyku': girdiler[8],
                                'televizyon': girdiler[9],
                                'eba_tv': girdiler[10],
                                'ders_calisma': girdiler[11],
                                'okul_dyk': girdiler[12],
                                'sosyal_kulturel': girdiler[13]}, ignore_index=True)

data_girdi

data_girdi=data_girdi.iloc[:,:].astype("int64")

val_y

girdi=data_girdi
print(girdi)
cikti=loaded_model.predict(girdi)
print(cikti)



cikti = cikti.astype("int64")
cikti

"""## Ort6 Regresyon Modeli"""

df_data = scaled_df.drop(["ort7"], axis=1)

x = scaled_df.drop([ "ort6", "ort7"], axis=1)
y = scaled_df["ort6"]

def backward_elimination(data, target,significance_level = 0.05):
  features = data.columns.tolist()
  while(len(features)>0):
    features_with_constant = sm.add_constant(data[features])
    p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]
    max_p_value = p_values.max()

    if(max_p_value >= significance_level):
      excluded_feature = p_values.idxmax()
      features.remove(excluded_feature)
    else:
      break 
  return features

ort6_futures = backward_elimination(x, y)

ort6_futures

"""### Regresyon Analizi"""

x = df_data[ort6_futures]
y = df_data["ort6"]

"""İstatistik modellerini kullandığımız için, sabit terimi X değerine eklememiz gerekiyor."""

x = sm.add_constant(x)

train_x, test_x, train_y, test_y = train_test_split(x, y, train_size = 0.8, random_state = 42)

model = sm.OLS(y, x)
model = model.fit()
print(model.summary2())

"""Değişkenlerin sonuca etki yüzdelerini inceleyelim;"""

coefs = model.params
coefs = pd.DataFrame(coefs, columns=["coefficients"])
coefs.drop(index=["const"], inplace=True)
coefs

coef_sum = coefs.coefficients.abs().sum()
coefs_abs = coefs.coefficients.abs().values
coefs_abs

percentages = []
for i in coefs_abs:
  perc = i*100/coef_sum
  percentages.append(perc)

coefs["Percentages"] = percentages
coefs.sort_values(by="Percentages", ascending=False, inplace=True)
print(coefs)

"""### Split Data"""

x = df[ort6_futures]
#x=x.drop(columns=["Aoz","ozel_kurs","sosyal_kulturel"])
y = df["ort6"]

y

train_x, val_x, train_y, val_y = train_test_split(x, y, random_state=42)

"""### Specify and Fit the Model"""

from sklearn.svm import SVR
model =SVR(C= 1000, gamma= 0.0001, kernel= 'rbf')

'''param_grid = { #when use hyperthread, xgboost may become slower
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [3,5, 6, 7],
              'n_estimators': [300,500,700]}

CV_rfc = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)
CV_rfc.fit(train_x, train_y)
print(CV_rfc.best_params_)'''

accuracies=cross_val_score(model,x,y,cv=5)
print("Accuracy (mean): %", accuracies.mean()*100)
print("std: %", accuracies.std()*100)
scores=cross_val_score(model,x,y,scoring='neg_mean_absolute_error',cv=5)
print("MAE (mean): %",scores.mean())

val_x

"""-0.49474651 -0.49474651 -0.37480796 -0.4047926  -0.46476187  2.23385544]"""

val_y

train_x.shape

type(val_x)

model.fit(train_x, train_y)

import pickle
model_path = "ort6.pickle"
pickle.dump(model, open(model_path, "wb"))

"""###  Make Predictions with Validation Data"""

load_model_path="ort6.pickle"
loaded_model = pickle.load(open(load_model_path, "rb"))

val_predictions = loaded_model.predict(val_x)

print("Eğitim doğruluğu: ", model.score(train_x, train_y)*100)
print("Test doğruluğu: ", model.score(val_x, val_y)*100)

val_predictions

#val_predictions = scaler_valy.inverse_transform(val_predictions)
val_predictions = val_predictions.astype("int64")
val_predictions

"""### Modelin manuel test edilmesi"""

load_model_path="ort6.pickle"
loaded_model = pickle.load(open(load_model_path, "rb"))

val_x.head()

val_y.head()

girdiler = [1,1,1,5,4,2,0,0,85]
data_girdi=pd.DataFrame(columns=['Bsag','Aoz','Abirlikte','Aogrenim','Bogrenim','ders_calisma','ozel_kurs','sosyal_kulturel','ort5'])
data_girdi = data_girdi.append({'Bsag': girdiler[0],'Aoz': girdiler[1], 'Abirlikte': girdiler[2], 'Aogrenim': girdiler[3], 'Bogrenim': girdiler[4], 'ders_calisma': girdiler[5],'ozel_kurs': girdiler[6], 'sosyal_kulturel': girdiler[7],
                                'ort5': girdiler[8]}, ignore_index=True)

data_girdi

data_girdi=data_girdi.iloc[:,:].astype("int64")

data_girdi.dtypes

girdi=data_girdi
print(girdi)
cikti=loaded_model.predict(girdi)
print(cikti)



cikti = cikti.astype("int64")
cikti



"""## Ort7 Regresyon Modeli"""

df_data = scaled_df

x = scaled_df.drop(["ort7"], axis=1)
y = scaled_df["ort7"]

def backward_elimination(data, target,significance_level = 0.05):
  features = data.columns.tolist()
  while(len(features)>0):
    features_with_constant = sm.add_constant(data[features])
    p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]
    max_p_value = p_values.max()

    if(max_p_value >= significance_level):
      excluded_feature = p_values.idxmax()
      features.remove(excluded_feature)
    else:
      break 
  return features

ort7_futures = backward_elimination(x, y)

ort7_futures

"""### Regresyon Analizi"""

x = df_data[ort7_futures]
y = df_data["ort7"]

"""İstatistik modellerini kullandığımız için, sabit terimi X değerine eklememiz gerekiyor."""

x = sm.add_constant(x)

train_x, test_x, train_y, test_y = train_test_split(x, y, train_size = 0.8, random_state = 42)

model = sm.OLS(y, x)
model = model.fit()
print(model.summary2())

"""Değişkenlerin sonuca etki yüzdelerini inceleyelim;"""

coefs = model.params
coefs = pd.DataFrame(coefs, columns=["coefficients"])
coefs.drop(index=["const"], inplace=True)
coefs

coef_sum = coefs.coefficients.abs().sum()
coefs_abs = coefs.coefficients.abs().values
coefs_abs

percentages = []
for i in coefs_abs:
  perc = i*100/coef_sum
  percentages.append(perc)

coefs["Percentages"] = percentages
coefs.sort_values(by="Percentages", ascending=False, inplace=True)
print(coefs)

"""### Split Data"""

x = df[ort7_futures]
y = df["ort7"]

train_x, val_x, train_y, val_y = train_test_split(x, y, random_state=42)

"""### Specify and Fit the Model"""

from sklearn.svm import SVR
model =SVR(C= 1000, gamma= 0.0001, kernel= 'rbf')

'''param_grid = {'kernel': ['rbf'], 'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5, 0.6, 0.9],'C': [1, 10, 100, 1000, 10000]}

CV_rfc = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)
CV_rfc.fit(train_x, train_y)
print(CV_rfc.best_params_)'''

accuracies=cross_val_score(model,x,y,cv=5)
print("Accuracy (mean): %", accuracies.mean()*100)
print("std: %", accuracies.std()*100)
scores=cross_val_score(model,x,y,scoring='neg_mean_absolute_error',cv=5)
print("MAE (mean): %",scores.mean())

val_x

val_y

model.fit(train_x, train_y)

import pickle
model_path = "ort7.pickle"
pickle.dump(model, open(model_path, "wb"))

"""###  Make Predictions with Validation Data"""

load_model_path="ort7.pickle"
loaded_model = pickle.load(open(load_model_path, "rb"))

val_predictions = loaded_model.predict(val_x)

print("Eğitim doğruluğu: ", model.score(train_x, train_y)*100)
print("Test doğruluğu: ", model.score(val_x, val_y)*100)

val_predictions

#val_predictions = scaler_valy.inverse_transform(val_predictions)
val_predictions = val_predictions.astype("int64")
val_predictions

"""### Modelin manuel test edilmesi"""

load_model_path="ort7.pickle"
loaded_model = pickle.load(open(load_model_path, "rb"))

val_x

girdiler = [1,5,4,1,61,66]
data_girdi=pd.DataFrame(columns=['Bsag','internet','oyun','ders_calisma','ort5','ort6'])
data_girdi = data_girdi.append({'Bsag': girdiler[0], 'internet': girdiler[1], 'oyun': girdiler[2] ,'ders_calisma': girdiler[3], 'ort5': girdiler[4], 'ort6': girdiler[5]}, ignore_index=True)

data_girdi

data_girdi=data_girdi.iloc[:,:].astype("int64")

data_girdi.dtypes

girdi=data_girdi
print(girdi)
cikti=loaded_model.predict(girdi)
print(cikti)



cikti = cikti.astype("int64")
cikti

val_y

